{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 밑바닥부터시작하는딥러닝1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 바른학습을 위해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 오버피팅 Overfitting\n",
    "- 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하는 상태\n",
    "- 매개변수가 많고 표현력이 높은 모델 / 훈련 데이터가 적음 => 오버피팅!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 가중치 감소\n",
    "- 학습과정에서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 오버피팅 억제\n",
    "- λ: 정규화의 세기를 조절하는 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 드롭아웃 Dropout\n",
    "- 뉴런을 임의로 삭제하면서 학습하는 방법\n",
    "- 훈련 때 은닉층의 뉴련을 무작위로 골라 삭제함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.mask: 훈련시에는 순전때마다 삭제할 뉴런을 False로 표시\n",
    "# self.mask: x와 형상이 같은 배열을 무작위로 생성\n",
    "# self.mask값이 dropout_ratio보다 큰 원소만 True로 설정\n",
    "# 순전파때 신호를 통과시키는 뉴련은 역전파 때도 신호를 그대로 통과시키고 순전파 때 통과시키지 않은 뉴련은 역전파때도 신호 차단\n",
    "\n",
    "class Dropout:\n",
    "    \n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, w, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 적절한 하이퍼파라미터 값 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 검증 데이터 Validation data\n",
    "- 시험 데이터를 사용해서는 안되는 이유? 시험데이터를 사용하여 하이퍼파라미터 조정하면 하이퍼파라미터값이 시험 데이터에 오버피팅됨\n",
    "- 그렇게 되면 다른 데이터에는 적응하지 못하니 범용 성능이 떨어지는 모델이 된다\n",
    "- 따라서 조정용 데이터 사용 => 검증 데이터 (Validation data)\n",
    "- 훈련데이터: 매개변수학습 / 검증데이터: 하이퍼파라미터성능평가 / 시험데이터: 신경망의 범용성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "# 훈련 데이터를 뒤섞는다\n",
    "def shuffle_dataset(x, t):\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "    return x, t\n",
    "\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 하이퍼파라미터 최적화\n",
    "- 하이퍼파라미터의 '최적값'이 존재하는 범위를 조금씩 줄여나감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 하이퍼파라미터 최적화 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 감소 계수\n",
    "weight_decay = 10**np.random.uniform(-8, -4)\n",
    "lr = 10**np.random.uniform(-6, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 CNN 전체 구조\n",
    "- 합성곱 계층 Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 합성곱 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 완전연결 계층의 문제점\n",
    "- 완전연결 계층의 문제점: 데이터 형상이 무시된다\n",
    "- 특징 맵(feature map): 합성곱 계층의 입출력 데이터\n",
    "- 입력 특징 맵(input feature map): 합성곱 계층의 입력 데이터\n",
    "- 출력 특징 맵(output feature map): 합성곱 계층의 출력 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 합성곱 연산\n",
    "- 필터(커널) 연산: 합성곱 연산\n",
    "- CNN에서는 필터의 매개변수 = 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 패딩 Padding\n",
    "- 패딩: 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값으로 채움\n",
    "- 주로 출력 크기를 조정할 목적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 스트라이드 stride\n",
    "#### 입력크기: (H, W), 필터크기: (FH, FW), 출력크기: (OH, OW), 패딩 P, 스트라이드: S\n",
    "### $OH={H+2P-FH\\over S} +1$\n",
    "### $OW={W+2P-FW\\over S} +1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5 3차원 데이터의 합성곱 연산\n",
    "- 주의할 점: 입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다\n",
    "- 필터 자체의 크기는 원하는 값으로 설정 가능, 단 모든 채널의 필터가 같은 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.6 블록으로 생각하기\n",
    "- (C, FH, FW) = (채널수, 필터 높이, 필터 너비)\n",
    "- 입력데이터 * 필터 = 출력데이터\n",
    "\n",
    "$(C, H, W) * (C, FH, FW) = (1, OH, OW)$\n",
    "\n",
    "- 연산의 출력으로 다수의 채널 내보내기, 필터를 FN 적용\n",
    "\n",
    "$(C, H, W) * (FN, C, FH, FW) = (FN, OH, OW)$\n",
    "\n",
    "- 편향 더해주기 (FN, 1, 1)\n",
    "\n",
    "$(C, H, W) * (FN, C, FH, FW) =>  (FN, OH, OW) + (FN, 1, 1) => (FN, OH, OW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.7 배치 처리\n",
    "- 처리 효율 높임\n",
    "- 4차원 데이터로 저장 (데이터 수, 채널 수, 높이, 너비)\n",
    "- 아래는 데이터가 N개일 때 배치 처리\n",
    "\n",
    "$(N, C, H, W) * (FN, C, FH, FW) =>  (N, FN, OH, OW) + (FN, 1, 1) => (N, FN, OH, OW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 풀링 계층 Pooling\n",
    "- 학습해야 할 매개변수가 없다\n",
    "- 채널 수가 변하지 않는다\n",
    "- 입력의 변화에 영향을 적게 받는다 (강건하다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 합성곱/풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 4차원 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (10, 1, 28, 28)\n",
      "x[0].shape: (1, 28, 28)\n",
      "x[1].shape: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(10, 1, 28, 28)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"x[0].shape:\", x[0].shape)\n",
    "print(\"x[1].shape:\", x[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 im2col로 데이터 전개\n",
    "- CNN은 4차원배열로 저장, 2차원 출력 데이터 -> 4차원으로 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 합성곱 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im2col(input_data, filter_hm filter_w, stride=1, pad=0)\n",
    "\n",
    "- input_data: (데이터수, 채널수, 높이, 너비)\n",
    "- filter_h: 필터의 높이\n",
    "- filter_w: 필터의 너비\n",
    "- stride: 스트라이드\n",
    "- pad: 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2col(input_data, filter_hm filter_w, stride=1, pad=0)\n",
    "# input_data: (데이터수, 채널수, 높이, 너비)\n",
    "# filter_h: 필터의 높이\n",
    "# filter_w: 필터의 너비\n",
    "# stride: 스트라이드\n",
    "# pad: 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10,3,7,7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, w, b, stride=1, pad=0):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.w.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH)/self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW)/self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_w) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 풀링 계층 구현하기\n",
    "1. 입력 데이터 전개\n",
    "2. 행병 최댓값 구하기\n",
    "3. 적절한 모양으로 성형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h)/self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w)/self,stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 CNN 구현하기\n",
    "- input_dim: 입력 데이터(채널수, 높이, 너비)의 차원\n",
    "- conv_param: 합성곱 계층의 하이퍼파라미터\n",
    "- hidden_size: 은닉층의 뉴런 수\n",
    "- output_size: 출력층의 뉴런 수\n",
    "- weight_init_std: 초기화 때의 가중치 표준편차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 CNN 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 밑바닥부터시작하는딥러닝2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 자연어 처리란\n",
    "- NLP: Natural Lanuage Processing 자연어를 처리하는 분야\n",
    "- 우리의 말을 컴퓨터에게 이해시키기 위한 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 시소러스\n",
    "- 시소러스 thesaurus: 유의어사전\n",
    "- 뜻이 같은 단어(동의어) = 뜻이 비슷한 단어(유의어)\n",
    "- 단어 네트워크를 이용해 컴퓨터에게 단어 사이의 관계 가르침\n",
    "\n",
    "### 2.2.1 WordNet\n",
    "\n",
    "### 2.2.2 시소러스의 문제점\n",
    "- 시대 변화에 대응하기 어렵다\n",
    "- 사람을 쓰는 비용은 크다\n",
    "- 단어의 미묘한 차이를 표현할 수 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 통계 기반 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 파이썬으로 말뭉치 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say goodbye and i say hello .\n",
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "text = text.lower()\n",
    "text = text.replace('.', ' .')\n",
    "print(text)\n",
    "\n",
    "words = text.split(' ')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "        \n",
    "print(id_to_word)\n",
    "print(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "            \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 단어의 분산 표현\n",
    "- 분산 표현: 단어의 의미를 정확하게 파악할 수 있는 벡터 표현\n",
    "\n",
    "### 2.3.3 분포 가설\n",
    "- 분포 가설: 단어의 의미는 주변 단어에 의해 형성된다\n",
    "- 즉 단어 자체에는 의미가 없고 그 단어가 사용된 '맥락'이 의미 형성\n",
    "- 맥락: 특정 단어를 중심에 둔 그 주변 단어\n",
    "- 윈도우 크기: 맥락의 크기 (주변 단어를 몇 개나 포함할지)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 동시발생행렬\n",
    "- 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법 (통계기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'preprocess' from 'common.util' (C:\\Users\\eunbi\\Desktop\\딥러닝의이론및실습\\common\\util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-19a6701c6397>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'You say goodbye and i say hello.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'preprocess' from 'common.util' (C:\\Users\\eunbi\\Desktop\\딥러닝의이론및실습\\common\\util.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and i say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 통계 기반 기법 개선하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
