{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 밑바닥부터시작하는딥러닝1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 바른학습을 위해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 오버피팅 Overfitting\n",
    "- 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하는 상태\n",
    "- 매개변수가 많고 표현력이 높은 모델 / 훈련 데이터가 적음 => 오버피팅!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 가중치 감소\n",
    "- 학습과정에서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 오버피팅 억제\n",
    "- λ: 정규화의 세기를 조절하는 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 드롭아웃 Dropout\n",
    "- 뉴런을 임의로 삭제하면서 학습하는 방법\n",
    "- 훈련 때 은닉층의 뉴련을 무작위로 골라 삭제함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.mask: 훈련시에는 순전때마다 삭제할 뉴런을 False로 표시\n",
    "# self.mask: x와 형상이 같은 배열을 무작위로 생성\n",
    "# self.mask값이 dropout_ratio보다 큰 원소만 True로 설정\n",
    "# 순전파때 신호를 통과시키는 뉴련은 역전파 때도 신호를 그대로 통과시키고 순전파 때 통과시키지 않은 뉴련은 역전파때도 신호 차단\n",
    "\n",
    "class Dropout:\n",
    "    \n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, w, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 적절한 하이퍼파라미터 값 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 검증 데이터 Validation data\n",
    "- 시험 데이터를 사용해서는 안되는 이유? 시험데이터를 사용하여 하이퍼파라미터 조정하면 하이퍼파라미터값이 시험 데이터에 오버피팅됨\n",
    "- 그렇게 되면 다른 데이터에는 적응하지 못하니 범용 성능이 떨어지는 모델이 된다\n",
    "- 따라서 조정용 데이터 사용 => 검증 데이터 (Validation data)\n",
    "- 훈련데이터: 매개변수학습 / 검증데이터: 하이퍼파라미터성능평가 / 시험데이터: 신경망의 범용성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "# 훈련 데이터를 뒤섞는다\n",
    "def shuffle_dataset(x, t):\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "    return x, t\n",
    "\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 하이퍼파라미터 최적화\n",
    "- 하이퍼파라미터의 '최적값'이 존재하는 범위를 조금씩 줄여나감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 하이퍼파라미터 최적화 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 감소 계수\n",
    "weight_decay = 10**np.random.uniform(-8, -4)\n",
    "lr = 10**np.random.uniform(-6, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 CNN 전체 구조\n",
    "- 합성곱 계층 Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 합성곱 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 완전연결 계층의 문제점\n",
    "- 완전연결 계층의 문제점: 데이터 형상이 무시된다\n",
    "- 특징 맵(feature map): 합성곱 계층의 입출력 데이터\n",
    "- 입력 특징 맵(input feature map): 합성곱 계층의 입력 데이터\n",
    "- 출력 특징 맵(output feature map): 합성곱 계층의 출력 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 합성곱 연산\n",
    "- 필터(커널) 연산: 합성곱 연산\n",
    "- CNN에서는 필터의 매개변수 = 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 패딩 Padding\n",
    "- 패딩: 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값으로 채움\n",
    "- 주로 출력 크기를 조정할 목적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 스트라이드 stride\n",
    "#### 입력크기: (H, W), 필터크기: (FH, FW), 출력크기: (OH, OW), 패딩 P, 스트라이드: S\n",
    "### $OH={H+2P-FH\\over S} +1$\n",
    "### $OW={W+2P-FW\\over S} +1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5 3차원 데이터의 합성곱 연산\n",
    "- 주의할 점: 입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다\n",
    "- 필터 자체의 크기는 원하는 값으로 설정 가능, 단 모든 채널의 필터가 같은 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.6 블록으로 생각하기\n",
    "- (C, FH, FW) = (채널수, 필터 높이, 필터 너비)\n",
    "- 입력데이터 * 필터 = 출력데이터\n",
    "\n",
    "$(C, H, W) * (C, FH, FW) = (1, OH, OW)$\n",
    "\n",
    "- 연산의 출력으로 다수의 채널 내보내기, 필터를 FN 적용\n",
    "\n",
    "$(C, H, W) * (FN, C, FH, FW) = (FN, OH, OW)$\n",
    "\n",
    "- 편향 더해주기 (FN, 1, 1)\n",
    "\n",
    "$(C, H, W) * (FN, C, FH, FW) =>  (FN, OH, OW) + (FN, 1, 1) => (FN, OH, OW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.7 배치 처리\n",
    "- 처리 효율 높임\n",
    "- 4차원 데이터로 저장 (데이터 수, 채널 수, 높이, 너비)\n",
    "- 아래는 데이터가 N개일 때 배치 처리\n",
    "\n",
    "$(N, C, H, W) * (FN, C, FH, FW) =>  (N, FN, OH, OW) + (FN, 1, 1) => (N, FN, OH, OW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 풀링 계층 Pooling\n",
    "- 학습해야 할 매개변수가 없다\n",
    "- 채널 수가 변하지 않는다\n",
    "- 입력의 변화에 영향을 적게 받는다 (강건하다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 합성곱/풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 4차원 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (10, 1, 28, 28)\n",
      "x[0].shape: (1, 28, 28)\n",
      "x[1].shape: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(10, 1, 28, 28)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"x[0].shape:\", x[0].shape)\n",
    "print(\"x[1].shape:\", x[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 im2col로 데이터 전개\n",
    "- CNN은 4차원배열로 저장, 2차원 출력 데이터 -> 4차원으로 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 합성곱 계층 구현하기\n",
    "##### im2col(input_data, filter_hm filter_w, stride=1, pad=0)\n",
    "\n",
    "- input_data: (데이터수, 채널수, 높이, 너비)\n",
    "- filter_h: 필터의 높이\n",
    "- filter_w: 필터의 너비\n",
    "- stride: 스트라이드\n",
    "- pad: 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2col(input_data, filter_hm filter_w, stride=1, pad=0)\n",
    "# input_data: (데이터수, 채널수, 높이, 너비)\n",
    "# filter_h: 필터의 높이\n",
    "# filter_w: 필터의 너비\n",
    "# stride: 스트라이드\n",
    "# pad: 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10,3,7,7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, w, b, stride=1, pad=0):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.w.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH)/self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW)/self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_w) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 풀링 계층 구현하기\n",
    "1. 입력 데이터 전개\n",
    "2. 행병 최댓값 구하기\n",
    "3. 적절한 모양으로 성형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h)/self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w)/self,stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 CNN 구현하기\n",
    "- input_dim: 입력 데이터(채널수, 높이, 너비)의 차원\n",
    "- conv_param: 합성곱 계층의 하이퍼파라미터\n",
    "- hidden_size: 은닉층의 뉴런 수\n",
    "- output_size: 출력층의 뉴런 수\n",
    "- weight_init_std: 초기화 때의 가중치 표준편차"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
